import { TopcamFb } from '@models/topcam-fb.entity';
import { User } from '@models/user.entity';
import { BadRequestException, Injectable, InternalServerErrorException, Logger } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import axios, { AxiosError } from 'axios';
import { FacebookAdsService } from 'src/facebook-ads/facebook-ads.service';
import { Repository } from 'typeorm';
import { MoreThanOrEqual } from 'typeorm';
import moment from 'moment';
import { FacebookPost } from '@models/facebook_post.entity';
import { randomUUID } from 'crypto';

@Injectable()
export class OpenaiService {
    private readonly logger = new Logger(OpenaiService.name);
    private readonly endpoint = 'https://api.openai.com/v1/chat/completions';
    private readonly model = 'gpt-5';
    private readonly timeout = 10 * 60 * 1000; // 10 ph√∫t

    // D√πng axios instance + interceptors ƒë·ªÉ log th·ªùi l∆∞·ª£ng & request-id
    private readonly http = axios.create({
        baseURL: this.endpoint.replace('/chat/completions', ''),
        timeout: this.timeout,
    });


    constructor(@InjectRepository(User) private readonly userRepo: Repository<User>,
        @InjectRepository(TopcamFb) private readonly topcamFbRepo: Repository<TopcamFb>,
        @InjectRepository(FacebookPost) private readonly repoFacebookPost: Repository<FacebookPost>,
        private readonly fbService: FacebookAdsService) {
        // mark start time
        this.http.interceptors.request.use((config) => {
            (config as any).__startedAt = Date.now();
            return config;
        });

        // log success (√≠t ·ªìn √†o)
        this.http.interceptors.response.use(
            (res) => {
                const started = (res.config as any).__startedAt ?? Date.now();
                const ms = Date.now() - started;
                const rid = res.headers?.['x-request-id'];
                this.logger.debug(`OpenAI OK ${res.status} (${ms}ms) model=${(res.data as any)?.model} reqId=${rid ?? '-'}`);
                return res;
            },
            (error: AxiosError) => {
                const started = (error.config as any)?.__startedAt ?? Date.now();
                (error as any).__durationMs = Date.now() - started;
                throw error;
            },
        );
    }

    /** üü¢ Ph√¢n t√≠ch targeting ‚Üí lu√¥n tr·∫£ JSON array */
    async analyzeTargeting(prompt: string, user: User) {
        console.log(`user in analyzeTargeting-------`, user);

        const userData = await this.userRepo
            .createQueryBuilder('user')
            .where('user.email = :email', { email: user?.email })
            .getOne();



        console.log(`userData in analyzeTargeting-------`, userData);
        if (!userData?.accountAdsId || !userData?.accessTokenUser) {
            throw new BadRequestException('User ch∆∞a c·∫•u h√¨nh Facebook Ads (accountAdsId ho·∫∑c accessTokenUser)');
        }

        const dataFacebookPosts = await this.repoFacebookPost
            .createQueryBuilder('p')
            .where(`split_part(p.post_id, '_', 1) = :pageId`, { pageId: userData.idPage })
            .getMany();

            console.log(`dataFacebookPosts in analyzeTargeting-------`, dataFacebookPosts);
            

        // Gom & lo·∫°i tr√πng keywords
        const uniqueKeywords = (() => {
            const all = dataFacebookPosts.flatMap(post =>
                Array.isArray(post?.dataTargeting?.keywordsForInterestSearch)
                    ? post.dataTargeting.keywordsForInterestSearch
                    : []
            );
            const cleaned = all.map(k => (k ?? '').trim()).filter(Boolean);
            const firstCaseMap = new Map<string, string>();
            for (const k of cleaned) {
                const lc = k.toLowerCase();
                if (!firstCaseMap.has(lc)) firstCaseMap.set(lc, k);
            }
            return [...firstCaseMap.values()].sort((a, b) => a.localeCompare(b));
        })();

        // G·∫Øn lu√¥n v√†o ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
        if (dataFacebookPosts[0]) {
            dataFacebookPosts[0].dataTargeting = {
                ...(dataFacebookPosts[0].dataTargeting ?? {}),
                keywordsForInterestSearch: uniqueKeywords,
            };
        }

        if (dataFacebookPosts[0]?.dataTargeting?.keywordsForInterestSearch.length > 0) {

            const first = dataFacebookPosts[0]?.dataTargeting ?? {};
            const result = first && Object.keys(first).length ? [first] : [];

            return {
                ok: true,
                result,
                raw: JSON.stringify(result, null, 2), // üëà gi·ªëng data 1 (string JSON pretty)
                usage: {
                    prompt_tokens: 0,
                    completion_tokens: 0,
                    total_tokens: 0,
                    prompt_tokens_details: {
                        cached_tokens: 0,
                        audio_tokens: 0,
                    },
                    completion_tokens_details: {
                        reasoning_tokens: 0,
                        audio_tokens: 0,
                        accepted_prediction_tokens: 0,
                        rejected_prediction_tokens: 0,
                    },
                },
                model: 'gpt-5-2025-08-07',
                requestId: `req_${typeof randomUUID === 'function'
                    ? randomUUID()
                    : Math.random().toString(36).slice(2)}`, // fallback n·∫øu thi·∫øu crypto
                status: 200,
            };
        }


        const todayStart = moment().startOf('day').toDate();
        const now = new Date();

        // ki·ªÉm tra ƒë√£ c√≥ d·ªØ li·ªáu update trong ng√†y ch∆∞a
        let topcamData = await this.topcamFbRepo.findOne({
            where: {
                userId: userData?.id,
                updatedAt: MoreThanOrEqual(todayStart),
            },
        });
        let top3Campaigns: any = [];

        if (!topcamData || !topcamData.topCam || (topcamData?.updatedAt < todayStart) || topcamData?.topCam?.length === 0 || topcamData?.topCam?.top3Campaigns?.length === 0 || topcamData?.topCam?.items?.length === 0) {
            const config = {
                apiVersion: 'v19.0',
                adAccountId: userData.accountAdsId,
                accessTokenUser: userData.accessTokenUser,
                cookie: userData.cookie,
            };

            const limit = '200';
            const fields = [
                `id`,
                `name`,
                `adset_id`,
                `campaign_id`,
                `status`,
                `effective_status`,
                `created_time`,
                `updated_time`,
            ];
            const effective_status = [`ACTIVE`, `PAUSED`, `ARCHIVED`];

            console.log(`Fetching top campaigns from Facebook...`, {
                limit: Math.max(1, parseInt(limit, 10)), // m·∫∑c ƒë·ªãnh 200
                fields,
                effective_status,
                apiVersion: config.apiVersion,
            },
                config,);


            top3Campaigns = await this.fbService.listAds(
                {
                    limit: Math.max(1, parseInt(limit, 10)), // m·∫∑c ƒë·ªãnh 200
                    fields,
                    effective_status,
                    apiVersion: config.apiVersion,
                },
                config,
            );

            console.log(`top3Campaigns-------`, JSON.stringify(top3Campaigns));

            // N·∫øu record ƒë√£ t·ªìn t·∫°i th√¨ update, n·∫øu ch∆∞a th√¨ insert
            topcamData = await this.topcamFbRepo.save({
                ...(topcamData || {}),
                userId: String(userData.id),
                topCam: top3Campaigns,
                updatedAt: now,
                // Ensure required fields are present
                id: topcamData?.id ?? undefined,
                generateBeforInsert: topcamData?.generateBeforInsert ?? null,
                doBeforUpdate: topcamData?.doBeforUpdate ?? null,
                createdAt: topcamData?.createdAt ?? new Date(),
            });
        }

        // return topcamData;

        console.log(`top3Campaigns-------`, JSON.stringify(top3Campaigns));

        const detailedPrompt = `${prompt} D·ª±a tr√™n c√°c chi·∫øn d·ªãch m·∫´u sau ƒë√¢y: ${JSON.stringify(top3Campaigns)}`


        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                'https://api.openai.com/v1/chat/completions',
                {
                    model: 'gpt-5', // ho·∫∑c 'gpt-5-turbo'
                    messages: [
                        {
                            role: 'system',
                            content: `B·∫°n l√† m√°y ph√¢n t√≠ch targeting. 
Ch·ªâ tr·∫£ v·ªÅ JSON H·ª¢P L·ªÜ (DUY NH·∫§T M·ªòT M·∫¢NG). 
C√°c key ph·∫£i vi·∫øt b·∫±ng ti·∫øng Vi·ªát c√≥ d·∫•u, ƒë√∫ng ch√≠nh t·∫£. 
Kh√¥ng tr·∫£ th√™m b·∫•t k·ª≥ k√Ω t·ª± n√†o kh√°c.`,
                        },
                        { role: 'user', content: detailedPrompt },
                    ],
                    max_completion_tokens: 4000, // ‚úÖ tham s·ªë m·ªõi
                },
                {
                    headers: {
                        Authorization: `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                    },
                },
            );


            const raw: string = data?.choices?.[0]?.message?.content ?? '[]';
            const arr = this.coerceToArray(raw);

            return {
                ok: true,
                result: arr,
                raw,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'analyzeTargeting', { promptLen: prompt?.length });
        }
    }


    /** üü¢ Copywriter ‚Üí tr·∫£ plain text */
    /** üü¢ Copywriter ‚Üí plain text (d√πng GPT-4) */
    async rewriteText(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post('/chat/completions',
                {
                    model: "gpt-4", // üëà ƒë·ªïi v·ªÅ GPT-4
                    messages: [
                        {
                            role: 'system',
                            content: 'B·∫°n l√† copywriter. H√£y ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n n·ªôi dung cu·ªëi c√πng, kh√¥ng k√®m gi·∫£i th√≠ch.',
                        },
                        { role: 'user', content: prompt },
                    ],
                    temperature: 0.4,  // üëà th√™m ƒë·ªÉ output ƒëa d·∫°ng h∆°n 1 ch√∫t
                    max_tokens: 4000,   // üëà GPT-4 d√πng max_tokens
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const newText = data?.choices?.[0]?.message?.content?.trim()
                || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c n·ªôi dung g·ª£i √Ω„Äë';

            return {
                ok: true,
                text: newText,
                usage: data?.usage || null,
                model: data?.model ?? "gpt-4",
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'rewriteText', { promptLen: prompt?.length });
        }
    }


    /** üü¢ Ch·∫•m ƒëi·ªÉm qu·∫£ng c√°o ‚Üí JSON array (d√πng GPT-4 cho ·ªïn ƒë·ªãnh) */
    async scoreAd(prompt: string) {
        this.logger.debug(`scoreAd prompt len=${prompt?.length}`);

        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post('/chat/completions',
                {
                    model: "gpt-4", // üëà ƒë·ªïi v·ªÅ GPT-4
                    messages: [
                        {
                            role: 'system',
                            content: 'B·∫°n l√† m√°y ch·∫•m ƒëi·ªÉm qu·∫£ng c√°o. Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá (m·ªôt M·∫¢NG), kh√¥ng th√™m ch·ªØ n√†o kh√°c.',
                        },
                        { role: 'user', content: prompt },
                    ],
                    temperature: 0,     // üëà ƒë·ªÉ output √≠t "n√≥i lan man"
                    max_tokens: 4000,   // üëà GPT-4 d√πng max_tokens
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const raw: string = data?.choices?.[0]?.message?.content ?? '[]';

            return {
                ok: true,
                result: raw,
                usage: data?.usage || null,
                model: data?.model ?? "gpt-4",
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'scoreAd', { promptLen: prompt?.length });
        }
    }


    /** üü¢ Helper: build headers */
    private buildHeaders(apiKey: string) {
        return {
            Authorization: `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            // G·∫Øn request id c·ªßa b·∫°n (t·ª± sinh) ƒë·ªÉ d√≤ log ƒë·∫ßu-cu·ªëi n·∫øu mu·ªën:
            // 'X-Request-Id': crypto.randomUUID(),
        };
    }

    /** üü¢ Helper parse array */
    private coerceToArray(text: any): any[] {
        if (typeof text !== 'string') return [];

        let cleaned = text.trim();
        cleaned = cleaned.replace(/^```(?:json)?/i, '').replace(/```$/i, '').trim();

        const firstBracket = cleaned.indexOf('[');
        const lastBracket = cleaned.lastIndexOf(']');
        if (firstBracket !== -1 && lastBracket !== -1) {
            cleaned = cleaned.slice(firstBracket, lastBracket + 1);
        }

        try {
            const parsed = JSON.parse(cleaned);
            return Array.isArray(parsed) ? parsed : [parsed];
        } catch (e) {
            this.logger.warn(`JSON parse failed, returning [] (len=${cleaned.length})`);
            return [];
        }
    }

    /** üü¢ Sinh n·ªôi dung general (GPT-4) */
    async generateText(prompt: any) {

        console.log(`prompt===============`, prompt);

        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) {
            throw new InternalServerErrorException('OPENAI_API_KEY is not set');
        }

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.7,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );


            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c n·ªôi dung g·ª£i √Ω„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'generateText', { messagesCount: prompt?.length });
        }
    }

    /** üü¢ Sinh ph·∫£n h·ªìi ƒë∆°n gi·∫£n t·ª´ prompt (GPT-4) */
    async simpleChat(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.7,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'simpleChat', { promptLen: prompt?.length });
        }
    }

    /** üü¢ Simple chat v·ªõi temperature cao (GPT-4) */
    async creativeChat(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.9,   // üëà creative h∆°n
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'creativeChat', { promptLen: prompt?.length });
        }
    }

    /** üü¢ Case ƒë·∫∑c th√π: ch·∫•m caption ch·ªâ tr·∫£ v·ªÅ M·ªòT CON S·ªê (0‚Äì100) */
    async scoreCaptionNumber(contentFetchOpportunityScore: string, captionText: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        const model = 'gpt-4';

        const messages = [
            { role: 'system', content: contentFetchOpportunityScore },
            {
                role: 'user',
                content: `${captionText}\n\nCh·∫•m theo thang 100 ƒëi·ªÉm. Ch·ªâ tr·∫£ l·ªùi b·∫±ng m·ªôt con s·ªë.`,
            },
        ];

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                { model, messages, temperature: 0.2, max_tokens: 4000 },
                { headers: this.buildHeaders(apiKey) },
            );

            const text = data?.choices?.[0]?.message?.content?.trim() ?? '';
            const match = text.match(/-?\d+(?:\.\d+)?/);
            const score = match ? Number(match[0]) : null;

            return {
                ok: score !== null,
                score,
                raw: text,
                usage: data?.usage || null,
                model: data?.model ?? model,
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'scoreCaptionNumber', {
                contentLen: contentFetchOpportunityScore?.length,
                captionLen: captionText?.length,
            });
        }
    }

    /** üü¢ D·ªãch & m·ªü r·ªông prompt sang ti·∫øng Anh */
    async translateAndExpandPrompt(text: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) {
            throw new InternalServerErrorException('OPENAI_API_KEY is not set');
        }

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [
                        {
                            role: 'user',
                            content: `Translate and expand into detailed English prompt: "${text}"`,
                        },
                    ],
                    temperature: 0.9,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const newPrompt =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c prompt„Äë';

            return {
                ok: true,
                prompt: newPrompt,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'translateAndExpandPrompt', { textLen: text?.length });
        }
    }

    /** üü¢ Vi·∫øt caption qu·∫£ng c√°o t·ª´ m√¥ t·∫£ s·∫£n ph·∫©m (system + user) */
    async generateCaptionFromDescription(contentGenerateCaption: string, description: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        const model = 'gpt-4'; // ho·∫∑c d√πng pickModel('gpt-4') n·∫øu b·∫°n ƒë√£ c√≥ helper
        const messages = [
            { role: 'system', content: contentGenerateCaption },
            { role: 'user', content: `M√¥ t·∫£ h√¨nh ·∫£nh s·∫£n ph·∫©m: "${description}". H√£y vi·∫øt m·ªôt caption qu·∫£ng c√°o theo ƒë√∫ng 10 ti√™u ch√≠ tr√™n.` },
        ];

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                { model, messages, temperature: 0.7, max_tokens: 500 },
                { headers: this.buildHeaders(apiKey) },
            );

            const caption =
                data?.choices?.[0]?.message?.content?.trim() || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c caption„Äë';

            return {
                ok: true,
                caption,
                usage: data?.usage || null,
                model: data?.model ?? model,
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'generateCaptionFromDescription', {
                sysLen: contentGenerateCaption?.length,
                descLen: description?.length,
            });
        }
    }

    async chatWithPrompt(promptContent: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: promptContent }],
                    temperature: 0.8,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage ?? null,
                model: data?.model ?? 'gpt-4',
                requestId: headers?.['x-request-id'] ?? null,
                status,
            };
        } catch (e) {
            const err = e as AxiosError<any>;
            const msg =
                err.response?.data?.error?.message ||
                err.response?.data?.message ||
                err.message ||
                'OpenAI request failed';
            const code = err.response?.status ?? 500;
            if (code === 400) throw new InternalServerErrorException(`OpenAI 400: ${msg}`);
            if (code === 401) throw new InternalServerErrorException('OpenAI 401: Invalid API key');
            if (code === 429) throw new InternalServerErrorException('OpenAI 429: Rate limit/quota');
            throw new InternalServerErrorException(`OpenAI ${code}: ${msg}`);
        }
    }







    /** üü¢ Helper: log & chu·∫©n ho√° l·ªói OpenAI */
    private handleOpenAiError(err: any, funcName: string, meta?: Record<string, any>): never {
        // Axios error shape
        const axErr = err as AxiosError<any>;
        const status = axErr.response?.status;
        const data = axErr.response?.data;
        const rid = axErr.response?.headers?.['x-request-id'];
        const durationMs = (axErr as any).__durationMs;

        // Th√¥ng ƒëi·ªáp t·ª´ OpenAI (n·∫øu c√≥)
        const oaiMsg: string | undefined =
            data?.error?.message ?? data?.message ?? axErr.message ?? 'OpenAI request failed';

        // Ph√¢n lo·∫°i m·ªôt s·ªë l·ªói m·∫°ng ph·ªï bi·∫øn
        const code = (axErr as any)?.code; // 'ECONNABORTED', 'ETIMEDOUT', 'ENOTFOUND', 'ECONNRESET'...
        const netHint =
            code === 'ECONNABORTED' ? 'Timeout' :
                code === 'ETIMEDOUT' ? 'Network timeout' :
                    code === 'ENOTFOUND' ? 'DNS not found' :
                        code === 'ECONNRESET' ? 'Socket reset' : undefined;

        // Log chi ti·∫øt cho DevOps
        this.logger.error(
            [
                `OpenAI ERROR in ${funcName}`,
                status ? `status=${status}` : '',
                code ? `code=${code}` : '',
                durationMs ? `duration=${durationMs}ms` : '',
                rid ? `reqId=${rid}` : '',
                meta ? `meta=${JSON.stringify(meta)}` : '',
                `msg="${oaiMsg}"`,
            ].filter(Boolean).join(' | '),
        );

        // Map sang HTTP exception cho controller
        if (status === 400) throw new BadRequestException(`OpenAI 400: ${oaiMsg}`);
        if (status === 401) throw new BadRequestException('OpenAI 401: Invalid API key or permissions');
        if (status === 403) throw new BadRequestException('OpenAI 403: Forbidden');
        if (status === 404) throw new BadRequestException('OpenAI 404: Not found (model/endpoint)');
        if (status === 408 || code === 'ECONNABORTED') throw new BadRequestException('OpenAI timeout, th·ª≠ gi·∫£m y√™u c·∫ßu ho·∫∑c tƒÉng timeout');
        if (status === 409) throw new BadRequestException('OpenAI 409: Conflict');
        if (status === 413) throw new BadRequestException('OpenAI 413: Payload qu√° l·ªõn (v∆∞·ª£t context)');
        if (status === 422) throw new BadRequestException('OpenAI 422: Unprocessable entity (tham s·ªë sai?)');
        if (status === 429) throw new BadRequestException('OpenAI 429: Rate limit ho·∫∑c h·∫øt quota');
        if (status === 500) throw new InternalServerErrorException('OpenAI 500: Internal error');
        if (status === 502 || status === 503 || status === 504) {
            throw new InternalServerErrorException(`OpenAI ${status}: Upstream unavailable`);
        }
        if (netHint) throw new InternalServerErrorException(`OpenAI network error: ${netHint}`);
        throw new InternalServerErrorException(`OpenAI error: ${oaiMsg}`);
    }
}
