import { BadRequestException, Injectable, InternalServerErrorException, Logger } from '@nestjs/common';
import axios, { AxiosError } from 'axios';

@Injectable()
export class OpenaiService {
    private readonly logger = new Logger(OpenaiService.name);

    private readonly endpoint = 'https://api.openai.com/v1/chat/completions';
    private readonly model = 'gpt-5';
    private readonly timeout = 10 * 60 * 1000; // 10 ph√∫t

    // D√πng axios instance + interceptors ƒë·ªÉ log th·ªùi l∆∞·ª£ng & request-id
    private readonly http = axios.create({
        baseURL: this.endpoint.replace('/chat/completions', ''),
        timeout: this.timeout,
    });

    constructor() {
        // mark start time
        this.http.interceptors.request.use((config) => {
            (config as any).__startedAt = Date.now();
            return config;
        });

        // log success (√≠t ·ªìn √†o)
        this.http.interceptors.response.use(
            (res) => {
                const started = (res.config as any).__startedAt ?? Date.now();
                const ms = Date.now() - started;
                const rid = res.headers?.['x-request-id'];
                this.logger.debug(`OpenAI OK ${res.status} (${ms}ms) model=${(res.data as any)?.model} reqId=${rid ?? '-'}`);
                return res;
            },
            (error: AxiosError) => {
                const started = (error.config as any)?.__startedAt ?? Date.now();
                (error as any).__durationMs = Date.now() - started;
                throw error;
            },
        );
    }

    /** üü¢ Ph√¢n t√≠ch targeting ‚Üí lu√¥n tr·∫£ JSON array */
    async analyzeTargeting(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post('/chat/completions',
                {
                    model: this.model,
                    messages: [
                        {
                            role: 'system',
                            content:
                                'B·∫°n l√† m√°y ph√¢n t√≠ch targeting. Ch·ªâ tr·∫£ v·ªÅ JSON H·ª¢P L·ªÜ (DUY NH·∫§T M·ªòT M·∫¢NG). Kh√¥ng tr·∫£ th√™m k√Ω t·ª± n√†o kh√°c.',
                        },
                        { role: 'user', content: prompt },
                    ],
                    // L∆∞u √Ω: v·ªõi c√°c model reasoning, d√πng `max_completion_tokens` l√† h·ª£p l·ªá (alias c·ªßa max_tokens). :contentReference[oaicite:0]{index=0}
                    max_completion_tokens: 4000,
                    // C√≥ th·ªÉ b·∫≠t ƒë·ªãnh d·∫°ng ch·∫∑t ch·∫Ω n·∫øu c·∫ßn:
                    response_format: { type: 'json_object' },
                },
                {
                    headers: this.buildHeaders(apiKey),
                },
            );

            const raw: string = data?.choices?.[0]?.message?.content ?? '[]';
            const arr = this.coerceToArray(raw);

            return {
                ok: true,
                result: arr,
                raw,
                usage: data?.usage || null,
                model: data?.model ?? this.model,
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'analyzeTargeting', { promptLen: prompt?.length });
        }
    }

    /** üü¢ Copywriter ‚Üí tr·∫£ plain text */
    /** üü¢ Copywriter ‚Üí plain text (d√πng GPT-4) */
    async rewriteText(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post('/chat/completions',
                {
                    model: "gpt-4", // üëà ƒë·ªïi v·ªÅ GPT-4
                    messages: [
                        {
                            role: 'system',
                            content: 'B·∫°n l√† copywriter. H√£y ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n n·ªôi dung cu·ªëi c√πng, kh√¥ng k√®m gi·∫£i th√≠ch.',
                        },
                        { role: 'user', content: prompt },
                    ],
                    temperature: 0.4,  // üëà th√™m ƒë·ªÉ output ƒëa d·∫°ng h∆°n 1 ch√∫t
                    max_tokens: 4000,   // üëà GPT-4 d√πng max_tokens
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const newText = data?.choices?.[0]?.message?.content?.trim()
                || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c n·ªôi dung g·ª£i √Ω„Äë';

            return {
                ok: true,
                text: newText,
                usage: data?.usage || null,
                model: data?.model ?? "gpt-4",
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'rewriteText', { promptLen: prompt?.length });
        }
    }


    /** üü¢ Ch·∫•m ƒëi·ªÉm qu·∫£ng c√°o ‚Üí JSON array (d√πng GPT-4 cho ·ªïn ƒë·ªãnh) */
    async scoreAd(prompt: string) {
        this.logger.debug(`scoreAd prompt len=${prompt?.length}`);

        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post('/chat/completions',
                {
                    model: "gpt-4", // üëà ƒë·ªïi v·ªÅ GPT-4
                    messages: [
                        {
                            role: 'system',
                            content: 'B·∫°n l√† m√°y ch·∫•m ƒëi·ªÉm qu·∫£ng c√°o. Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá (m·ªôt M·∫¢NG), kh√¥ng th√™m ch·ªØ n√†o kh√°c.',
                        },
                        { role: 'user', content: prompt },
                    ],
                    temperature: 0,     // üëà ƒë·ªÉ output √≠t "n√≥i lan man"
                    max_tokens: 4000,   // üëà GPT-4 d√πng max_tokens
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const raw: string = data?.choices?.[0]?.message?.content ?? '[]';

            return {
                ok: true,
                result: raw,
                usage: data?.usage || null,
                model: data?.model ?? "gpt-4",
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'scoreAd', { promptLen: prompt?.length });
        }
    }


    /** üü¢ Helper: build headers */
    private buildHeaders(apiKey: string) {
        return {
            Authorization: `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            // G·∫Øn request id c·ªßa b·∫°n (t·ª± sinh) ƒë·ªÉ d√≤ log ƒë·∫ßu-cu·ªëi n·∫øu mu·ªën:
            // 'X-Request-Id': crypto.randomUUID(),
        };
    }

    /** üü¢ Helper parse array */
    private coerceToArray(text: any): any[] {
        if (typeof text !== 'string') return [];

        let cleaned = text.trim();
        cleaned = cleaned.replace(/^```(?:json)?/i, '').replace(/```$/i, '').trim();

        const firstBracket = cleaned.indexOf('[');
        const lastBracket = cleaned.lastIndexOf(']');
        if (firstBracket !== -1 && lastBracket !== -1) {
            cleaned = cleaned.slice(firstBracket, lastBracket + 1);
        }

        try {
            const parsed = JSON.parse(cleaned);
            return Array.isArray(parsed) ? parsed : [parsed];
        } catch (e) {
            this.logger.warn(`JSON parse failed, returning [] (len=${cleaned.length})`);
            return [];
        }
    }

    /** üü¢ Sinh n·ªôi dung general (GPT-4) */
    async generateText(prompt: any) {

        console.log(`prompt===============`, prompt);

        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) {
            throw new InternalServerErrorException('OPENAI_API_KEY is not set');
        }

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.7,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );


            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c n·ªôi dung g·ª£i √Ω„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'generateText', { messagesCount: prompt?.length });
        }
    }

    /** üü¢ Sinh ph·∫£n h·ªìi ƒë∆°n gi·∫£n t·ª´ prompt (GPT-4) */
    async simpleChat(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.7,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'simpleChat', { promptLen: prompt?.length });
        }
    }

    /** üü¢ Simple chat v·ªõi temperature cao (GPT-4) */
    async creativeChat(prompt: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: prompt }],
                    temperature: 0.9,   // üëà creative h∆°n
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'creativeChat', { promptLen: prompt?.length });
        }
    }

    /** üü¢ Case ƒë·∫∑c th√π: ch·∫•m caption ch·ªâ tr·∫£ v·ªÅ M·ªòT CON S·ªê (0‚Äì100) */
    async scoreCaptionNumber(contentFetchOpportunityScore: string, captionText: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        const model = 'gpt-4';

        const messages = [
            { role: 'system', content: contentFetchOpportunityScore },
            {
                role: 'user',
                content: `${captionText}\n\nCh·∫•m theo thang 100 ƒëi·ªÉm. Ch·ªâ tr·∫£ l·ªùi b·∫±ng m·ªôt con s·ªë.`,
            },
        ];

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                { model, messages, temperature: 0.2, max_tokens: 4000 },
                { headers: this.buildHeaders(apiKey) },
            );

            const text = data?.choices?.[0]?.message?.content?.trim() ?? '';
            const match = text.match(/-?\d+(?:\.\d+)?/);
            const score = match ? Number(match[0]) : null;

            return {
                ok: score !== null,
                score,
                raw: text,
                usage: data?.usage || null,
                model: data?.model ?? model,
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'scoreCaptionNumber', {
                contentLen: contentFetchOpportunityScore?.length,
                captionLen: captionText?.length,
            });
        }
    }

    /** üü¢ D·ªãch & m·ªü r·ªông prompt sang ti·∫øng Anh */
    async translateAndExpandPrompt(text: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) {
            throw new InternalServerErrorException('OPENAI_API_KEY is not set');
        }

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [
                        {
                            role: 'user',
                            content: `Translate and expand into detailed English prompt: "${text}"`,
                        },
                    ],
                    temperature: 0.9,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const newPrompt =
                data?.choices?.[0]?.message?.content?.trim() ||
                '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c prompt„Äë';

            return {
                ok: true,
                prompt: newPrompt,
                usage: data?.usage || null,
                model: data?.model ?? 'gpt-4',
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'translateAndExpandPrompt', { textLen: text?.length });
        }
    }

    /** üü¢ Vi·∫øt caption qu·∫£ng c√°o t·ª´ m√¥ t·∫£ s·∫£n ph·∫©m (system + user) */
    async generateCaptionFromDescription(contentGenerateCaption: string, description: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        const model = 'gpt-4'; // ho·∫∑c d√πng pickModel('gpt-4') n·∫øu b·∫°n ƒë√£ c√≥ helper
        const messages = [
            { role: 'system', content: contentGenerateCaption },
            { role: 'user', content: `M√¥ t·∫£ h√¨nh ·∫£nh s·∫£n ph·∫©m: "${description}". H√£y vi·∫øt m·ªôt caption qu·∫£ng c√°o theo ƒë√∫ng 10 ti√™u ch√≠ tr√™n.` },
        ];

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                { model, messages, temperature: 0.7, max_tokens: 500 },
                { headers: this.buildHeaders(apiKey) },
            );

            const caption =
                data?.choices?.[0]?.message?.content?.trim() || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c caption„Äë';

            return {
                ok: true,
                caption,
                usage: data?.usage || null,
                model: data?.model ?? model,
                requestId: headers['x-request-id'] ?? null,
                status,
            };
        } catch (err: any) {
            this.handleOpenAiError(err, 'generateCaptionFromDescription', {
                sysLen: contentGenerateCaption?.length,
                descLen: description?.length,
            });
        }
    }

    async chatWithPrompt(promptContent: string) {
        const apiKey = process.env.OPENAI_API_KEY;
        if (!apiKey) throw new InternalServerErrorException('OPENAI_API_KEY is not set');

        try {
            const { data, headers, status } = await this.http.post(
                '/chat/completions',
                {
                    model: 'gpt-4',
                    messages: [{ role: 'user', content: promptContent }],
                    temperature: 0.8,
                    max_tokens: 4000,
                },
                { headers: this.buildHeaders(apiKey) },
            );

            const text =
                data?.choices?.[0]?.message?.content?.trim() || '„ÄêKh√¥ng t·∫°o ƒë∆∞·ª£c ph·∫£n h·ªìi„Äë';

            return {
                ok: true,
                text,
                usage: data?.usage ?? null,
                model: data?.model ?? 'gpt-4',
                requestId: headers?.['x-request-id'] ?? null,
                status,
            };
        } catch (e) {
            const err = e as AxiosError<any>;
            const msg =
                err.response?.data?.error?.message ||
                err.response?.data?.message ||
                err.message ||
                'OpenAI request failed';
            const code = err.response?.status ?? 500;
            if (code === 400) throw new InternalServerErrorException(`OpenAI 400: ${msg}`);
            if (code === 401) throw new InternalServerErrorException('OpenAI 401: Invalid API key');
            if (code === 429) throw new InternalServerErrorException('OpenAI 429: Rate limit/quota');
            throw new InternalServerErrorException(`OpenAI ${code}: ${msg}`);
        }
    }







    /** üü¢ Helper: log & chu·∫©n ho√° l·ªói OpenAI */
    private handleOpenAiError(err: any, funcName: string, meta?: Record<string, any>): never {
        // Axios error shape
        const axErr = err as AxiosError<any>;
        const status = axErr.response?.status;
        const data = axErr.response?.data;
        const rid = axErr.response?.headers?.['x-request-id'];
        const durationMs = (axErr as any).__durationMs;

        // Th√¥ng ƒëi·ªáp t·ª´ OpenAI (n·∫øu c√≥)
        const oaiMsg: string | undefined =
            data?.error?.message ?? data?.message ?? axErr.message ?? 'OpenAI request failed';

        // Ph√¢n lo·∫°i m·ªôt s·ªë l·ªói m·∫°ng ph·ªï bi·∫øn
        const code = (axErr as any)?.code; // 'ECONNABORTED', 'ETIMEDOUT', 'ENOTFOUND', 'ECONNRESET'...
        const netHint =
            code === 'ECONNABORTED' ? 'Timeout' :
                code === 'ETIMEDOUT' ? 'Network timeout' :
                    code === 'ENOTFOUND' ? 'DNS not found' :
                        code === 'ECONNRESET' ? 'Socket reset' : undefined;

        // Log chi ti·∫øt cho DevOps
        this.logger.error(
            [
                `OpenAI ERROR in ${funcName}`,
                status ? `status=${status}` : '',
                code ? `code=${code}` : '',
                durationMs ? `duration=${durationMs}ms` : '',
                rid ? `reqId=${rid}` : '',
                meta ? `meta=${JSON.stringify(meta)}` : '',
                `msg="${oaiMsg}"`,
            ].filter(Boolean).join(' | '),
        );

        // Map sang HTTP exception cho controller
        if (status === 400) throw new BadRequestException(`OpenAI 400: ${oaiMsg}`);
        if (status === 401) throw new BadRequestException('OpenAI 401: Invalid API key or permissions');
        if (status === 403) throw new BadRequestException('OpenAI 403: Forbidden');
        if (status === 404) throw new BadRequestException('OpenAI 404: Not found (model/endpoint)');
        if (status === 408 || code === 'ECONNABORTED') throw new BadRequestException('OpenAI timeout, th·ª≠ gi·∫£m y√™u c·∫ßu ho·∫∑c tƒÉng timeout');
        if (status === 409) throw new BadRequestException('OpenAI 409: Conflict');
        if (status === 413) throw new BadRequestException('OpenAI 413: Payload qu√° l·ªõn (v∆∞·ª£t context)');
        if (status === 422) throw new BadRequestException('OpenAI 422: Unprocessable entity (tham s·ªë sai?)');
        if (status === 429) throw new BadRequestException('OpenAI 429: Rate limit ho·∫∑c h·∫øt quota');
        if (status === 500) throw new InternalServerErrorException('OpenAI 500: Internal error');
        if (status === 502 || status === 503 || status === 504) {
            throw new InternalServerErrorException(`OpenAI ${status}: Upstream unavailable`);
        }
        if (netHint) throw new InternalServerErrorException(`OpenAI network error: ${netHint}`);
        throw new InternalServerErrorException(`OpenAI error: ${oaiMsg}`);
    }
}
